# Cifar-10 Training Result

标签 ： Experiment

---

###第一阶段：学习数据的相关因素对于整个学习效果的影响
####结论：
* 对于cifar数据集，单次学习的效果明显是优于多次学习，但是对于MNIST数据集，单次学习的效果是低于多次学习的效果的，两者之间差别产生的原因还需要进一步研究
* 随着单次训练的batch数据集的增加分类效果增长到了一定的地步后，不会继续增长，说明单层的softmax网络性能是不够好的。
* 多次学习cifar数据集的结果稳定在0.1

####学习：
* 数据的相关处理：预处理：减去均值，保证数据的正规化，具体作用需要进一步了解

---

###第二阶段：改写代码（模块化）、搭建CNN网络
本次改写代码，将onehot-encoding、数据预处理等部分写成函数，同时搭建了CNN网络（隐含层数分别为一层、二层、三层）。上次实验得到的正确率只有10%左右，经过这一次的分析容易知道问题主要在于两个方面：

* 数据的预处理有问题，对于RGB的图像，进行预处理时应该利用(R-128)/128、(B-128)/128、(G-128)/128，而不是除以平均值
* 对于训练的步长也应该进行恰当的选择，步长不能过长，否则很容易陷入局部最优，训练不完全。
* 一次batch实验的数据应该是足够的（不能过大过小，否则速度过慢也可能带来更大的误差->一次需要符合的数据太多，无法兼顾）

经过以上的经验，本次试验则是将batch的容量保证在128，学习100k次，学习速率保证在0.1，然后从新进行训练。

* 隐含层数为0
网络相关参数：输入32x32x3的图片，输出长度为10的向量
准确率：10%（由于总共也只有10个类，等概率的情况下刚好是0.1）
* 隐含层数为1
网络相关参数：5x5的卷积核（按照等大小产生图片）、2x2的池化尺寸（长宽均缩小为原来的一半）、一层密集连接层（连接元的个数会影响最后的准确率，所得到的结论如下：）

| 密集层连接元个数 | 准确率 |
|:----------------:|:------:|
|128|0.5911|
|256|0.5997|
|512|0.6095|

不难发现连接元的个数按指数增长，准确率的增加却是对数级别的。

* 隐含层数为2
网络相关参数：两层5x5的卷积核（按照等大小产生图片）和2x2的池化尺寸（长宽均缩小为原来的一半），一层密集连接层（512个连接元）
准确率：0.6536
* 隐含层数为3
网络相关参数：三层5x5的卷积核（按照等大小产生图片）和2x2的池化尺寸（长宽均缩小为原来的一半），一层密集连接层（512个连接元）
准确率：0.6561

---

####第三阶段：保证程序正确性，提高正确率，减轻过拟合
#####*以下工作都是在只有一层卷积层的基础上进行的*

**Part 0：更改数据处理方式**

将原来的减去128，更改成为减去所有训练数据在RGB三个通道的1024个像素上的平均值

| 数据处理方式| 结果正确率 |
|:---------:|:----------:|
| 减去128 | 64.8% |
| 减去平均 |  |

**Part 1：修改Dropout**

修改了Dropout的值，当计算accuracy时的dropout从0.5改为0时(训练的Dropout仍为0.5)，正确率为64.8%。当修改了最后一层全连接与softmax之间的Dropout值，结果正确率变化情况如下：

| 训练的Dropout大小 | 结果正确率 |
|:-----------------:|:----------:|
| 0.5 | 64.8% |
| 0.6 | 64.8% |
| 0.7 | 63% |

**结论：**对于最后一层的Dropout的改变对于最后的结果影响较小

| 隐含层dropout | 正确率 |
|:---:|:---:|
| 0.65 | 63.7% |
| 0.70 | 63.6% |
| 0.80 | 65.2% |
| 0.90 | 65.4% |
|

**结论：**对于隐含层层添加Dropout可以有效地增加网络的准确性，同时也对过拟合有一定的缓解作用。











